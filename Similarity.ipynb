{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-traitement du corpus(enlever les ponctuation et lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shailynnxie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/shailynnxie/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "df = pd.read_csv('/Users/shailynnxie/Documents/M2/Logicielle/ubuntu_sample.tsv', sep='\\t', header=None)\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "df.iloc[:, 1] = df.iloc[:, 1].apply(preprocess_text)\n",
    "df.iloc[:, 2] = df.iloc[:, 2].apply(preprocess_text)\n",
    "\n",
    "df.to_csv('/Users/shailynnxie/Documents/M2/Logicielle/processed_file.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization du librairie NLTK n'est pas parfait\n",
    "\n",
    "Il ne peut pas lemmatizer le temps du verbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did\n",
      "doing\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('did'))\n",
    "print(lemmatizer.lemmatize('doing'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF avec 2 gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('/Users/shailynnxie/Documents/M2/Logicielle/processed_file.csv')\n",
    "\n",
    "# obtenir question et answer\n",
    "texts_2 = df.iloc[:, 1].values.tolist()\n",
    "texts_3 = df.iloc[:, 2].values.tolist()\n",
    "\n",
    "# Créez un vectoriseur TF-IDF et définissez ngram_range sur (1, 2) pour utiliser uni-gramme et bi-gramme\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "#vectorier les answers\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(texts_3)\n",
    "\n",
    "\n",
    "matches = []\n",
    "\n",
    "# match les answers avec chaque question\n",
    "for text in texts_2:\n",
    "\n",
    "    text_tfidf = tfidf_vectorizer.transform([text])\n",
    "    similarity_scores = cosine_similarity(text_tfidf, tfidf_matrix)\n",
    "    best_match_index = similarity_scores.argmax()\n",
    "    matches.append(texts_3[best_match_index])\n",
    "\n",
    "df['best_match'] = matches\n",
    "\n",
    "df.to_csv('/Users/shailynnxie/Documents/M2/Logicielle/TFIDF 2gram.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Aucuracy Rate: 63.10%\n"
     ]
    }
   ],
   "source": [
    "column3 = df.iloc[:, 2]\n",
    "column4 = df.iloc[:, 3]\n",
    "\n",
    "    \n",
    "consistent_count = 0\n",
    "\n",
    "\n",
    "for value3, value4 in zip(column3, column4):\n",
    "        if value3 == value4:\n",
    "            consistent_count += 1\n",
    "\n",
    "\n",
    "total_rows = len(df)\n",
    "consistency_rate = consistent_count / total_rows\n",
    "print(f\" Aucuracy Rate: {consistency_rate:.2%}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### le résultat est un peu moins bien que l'autre fois(sans prétraitement (65.90%))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIF similarité\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import gensim.downloader as api\n",
    "\n",
    "#loading un model pour entrainement\n",
    "glove_vectors = api.load('glove-twitter-200')\n",
    "\n",
    "# calculer SIF feature et vectoriser\n",
    "def map_word_frequency(document):\n",
    "    return Counter(itertools.chain(*document))\n",
    "\n",
    "def get_sif_feature_vectors(texts, word_emb_model):\n",
    "    sif_weighted_embeddings = []\n",
    "    for text in texts:\n",
    "\n",
    "        tokens = text.split()\n",
    "\n",
    "        tokens = [token for token in tokens if token in word_emb_model]\n",
    "\n",
    "        word_counts = map_word_frequency([tokens])\n",
    "\n",
    "        sentence_vec = np.zeros(word_emb_model.vector_size)\n",
    "        for word in tokens:\n",
    "            # calcul SIF-weighted \n",
    "            alpha = 0.001\n",
    "            weight = alpha / (alpha + word_counts[word])\n",
    "            sentence_vec += weight * word_emb_model[word]\n",
    "        # normalization\n",
    "        sentence_vec /= len(tokens)\n",
    "        \n",
    "        sif_weighted_embeddings.append(sentence_vec)\n",
    "    return sif_weighted_embeddings\n",
    "\n",
    "df = pd.read_csv('/Users/shailynnxie/Documents/M2/Logicielle/processed_file.csv')\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "\n",
    "texts_2 = df.iloc[:, 1].values.tolist()\n",
    "texts_3 = df.iloc[:, 2].values.tolist()\n",
    "\n",
    "sif_vectors_2 = get_sif_feature_vectors(texts_2, glove_vectors)\n",
    "sif_vectors_3 = get_sif_feature_vectors(texts_3, glove_vectors)\n",
    "\n",
    "# transférer les vecter aux matrix NumPy\n",
    "sif_vectors_2 = np.array(sif_vectors_2)\n",
    "sif_vectors_3 = np.array(sif_vectors_3)\n",
    "for dist_name in [\"jaccard\", \"euclidean\",\"manhattan\", \"cosine\"]:\n",
    "    distances = pairwise_distances(sif_vectors_2, sif_vectors_3,metric=dist_name)\n",
    "\n",
    "    min_distances_indices = np.argmin(distances, axis=1)\n",
    "    matched_answers = df.iloc[min_distances_indices, 2].values\n",
    "    similarity_scores = 1 - np.min(distances, axis=1)\n",
    "\n",
    "    \n",
    "    df['best_match'] = matched_answers\n",
    "    df['similarity_score'] = similarity_scores\n",
    "\n",
    "    df.to_csv('/Users/shailynnxie/Documents/M2/Logicielle/SIF_similarity_%s.csv'% dist_name, index=False)\n",
    "\n",
    "#print(distances)\n",
    "#print(distances.shape)\n",
    "#print(matched_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard  Aucuracy Rate: 0.10%\n",
      "euclidean  Aucuracy Rate: 13.30%\n",
      "manhattan  Aucuracy Rate: 17.50%\n",
      "cosine  Aucuracy Rate: 23.20%\n"
     ]
    }
   ],
   "source": [
    "for dist_name in [\"jaccard\", \"euclidean\",\"manhattan\", \"cosine\"]:\n",
    "\n",
    "\n",
    "    df = pd.read_csv(\"/Users/shailynnxie/Documents/M2/Logicielle/SIF_similarity_%s.csv\" % dist_name)\n",
    "\n",
    "    # comparer si l'étément dans la colone 2 et 3 sont identiques\n",
    "    column3 = df.iloc[:, 2]\n",
    "    column4 = df.iloc[:, 3]\n",
    "\n",
    "    \n",
    "    consistent_count = 0\n",
    "\n",
    "\n",
    "    for value3, value4 in zip(column3, column4):\n",
    "        if value3 == value4:\n",
    "            consistent_count += 1\n",
    "\n",
    "\n",
    "    total_rows = len(df)\n",
    "    consistency_rate = consistent_count / total_rows\n",
    "\n",
    "    print(dist_name,f\" Aucuracy Rate: {consistency_rate:.2%}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarité euclidean et  manhattan sont améliorer comparant dernière fois, mais jaccard et cosine sont beaucoup dégradés\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Aucuracy Rate: 27.80%\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/shailynnxie/Documents/M2/Logicielle/matched_results.csv\" )\n",
    "\n",
    "# comparer si l'étément dans la colone 2 et 3 sont identiques\n",
    "column3 = df.iloc[:, 1]\n",
    "column4 = df.iloc[:, 2]\n",
    "\n",
    "\n",
    "consistent_count = 0\n",
    "\n",
    "\n",
    "for value3, value4 in zip(column3, column4):\n",
    "    if value3 == value4:\n",
    "        consistent_count += 1\n",
    "\n",
    "\n",
    "total_rows = len(df)\n",
    "consistency_rate = consistent_count / total_rows\n",
    "\n",
    "print(f\" Aucuracy Rate: {consistency_rate:.2%}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L'algorithme Hongrois"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conbinaison avec TF-IDF pondération"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Rate: 76.70%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "# Charger le fichier CSV prétraité\n",
    "df = pd.read_csv('/Users/shailynnxie/Documents/M2/Logicielle/processed_file.csv')\n",
    "df.fillna('', inplace=True)# Remplacer les valeurs manquantes par des chaînes vides\n",
    "\n",
    "V = TfidfVectorizer()# Initialisation du vectorisateur TF-IDF\n",
    "\n",
    "# Vectorisation des réponses (colonne 3) avec TF-IDF\n",
    "answers_vectors = V.fit_transform(df.iloc[:, 2]).toarray()\n",
    "\n",
    "# Calcul de la matrice des distances entre les questions (colonne 2) et les réponses vectorisées\n",
    "# Utilisation de la distance cosinus\n",
    "distances_matrix = pairwise_distances(V.transform(df.iloc[:, 1]).toarray(), answers_vectors, metric='cosine')\n",
    "\n",
    "# Application de l'algorithme hongrois pour trouver le meilleur appariement entre questions et réponses\n",
    "\n",
    "row_indices, col_indices = linear_sum_assignment(distances_matrix)\n",
    "# Extraction des réponses correspondantes aux indices trouvés par l'algorithme\n",
    "\n",
    "matched_answers = df.iloc[col_indices, 2].values\n",
    "actual_answers = df.iloc[:, 2].values\n",
    "\n",
    "# calculer exactitude\n",
    "correct_matches = (actual_answers == matched_answers).sum()\n",
    "accuracy_rate = correct_matches / len(df)\n",
    "\n",
    "print(f\"Accuracy Rate: {accuracy_rate:.2%}\")\n",
    "\n",
    "# Sauvegarde des résultats dans un nouveau fichier CSV pour voir les q&a qui sont bien matche\n",
    "results_df.to_csv(\"C:/Users/64271/Desktop/gael/matching_results_with_accuracy.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le fichier CSV prétraité\n",
    "df = pd.read_csv('C:/Users/64271/Desktop/gael/processed_file.csv')\n",
    "df.fillna('', inplace=True)  # Remplacer les valeurs manquantes par des chaînes vides\n",
    "\n",
    "# Initialisation du vectorisateur TF-IDF\n",
    "V = TfidfVectorizer()\n",
    "\n",
    "# Vectorisation des réponses (colonne 3) avec TF-IDF\n",
    "answers_vectors = V.fit_transform(df.iloc[:, 2]).toarray()\n",
    "\n",
    "# Calcul de la matrice des distances entre les questions (colonne 2) et les réponses vectorisées\n",
    "# Utilisation de la distance cosinus\n",
    "distances_matrix = pairwise_distances(V.transform(df.iloc[:, 1]).toarray(), answers_vectors, metric='cosine')\n",
    "\n",
    "# Application de l'algorithme hongrois pour trouver le meilleur appariement entre questions et réponses\n",
    "row_indices, col_indices = linear_sum_assignment(distances_matrix)\n",
    "\n",
    "# Extraction des réponses correspondantes aux indices trouvés par l'algorithme\n",
    "matched_answers = df.iloc[col_indices, 2].values\n",
    "actual_answers = df.iloc[:, 2].values  # Les réponses actuelles (supposées être dans la colonne 3)\n",
    "\n",
    "# Calcul du taux de précision en comparant les réponses actuelles aux réponses appariées\n",
    "correct_matches = (actual_answers == matched_answers).sum()\n",
    "accuracy_rate = correct_matches / len(df)\n",
    "\n",
    "# Création d'un nouveau DataFrame pour stocker les questions, les réponses actuelles, les réponses appariées\n",
    "# et si l'appariement est correct\n",
    "results_df = pd.DataFrame({\n",
    "    'Question': df.iloc[:, 1],  #la colonne 2 contient les questions\n",
    "    'Actual Answer': actual_answers,  # Réponses actuelles\n",
    "    'Matched Answer': matched_answers,  # Réponses appariées par l'algorithme\n",
    "    'Is Correct': actual_answers == matched_answers  # Indique si l'appariement est correct\n",
    "})\n",
    "\n",
    "# Calcul et affichage du taux de précision\n",
    "accuracy_rate = results_df['Is Correct'].mean()\n",
    "print(f\"Taux de Précision: {accuracy_rate:.2%}\")\n",
    "\n",
    "# Sauvegarde des résultats dans un nouveau fichier CSV pour voir les q&a qui sont bien matche\n",
    "results_df.to_csv(\"C:/Users/64271/Desktop/gael/matching_results_with_accuracy.csv\", index=False)\n",
    "\n",
    "# Affichage du chemin du fichier pour connaître l'emplacement du fichier sauvegardé\n",
    "\"C:/Users/64271/Desktop/gael/matching_results_with_accuracy.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conbinaison avec SIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "texts_2 = df.iloc[:, 1].values.tolist()\n",
    "texts_3 = df.iloc[:, 2].values.tolist()\n",
    "\n",
    "sif_vectors_2 = get_sif_feature_vectors(texts_2, glove_vectors)\n",
    "sif_vectors_3 = get_sif_feature_vectors(texts_3, glove_vectors)\n",
    "\n",
    "# transférer les vecter aux matrix NumPy\n",
    "sif_vectors_2 = np.array(sif_vectors_2)\n",
    "sif_vectors_3 = np.array(sif_vectors_3)\n",
    "# Calculer la distance entre les vecteurs SIF des textes des colonnes 2 et 3\n",
    "distances = cosine_similarity(sif_vectors_2, sif_vectors_3)\n",
    "\n",
    "# Utiliser la méthode Hungarian (ou de Kuhn-Munkres) pour trouver les meilleures correspondances\n",
    "row_ind, col_ind = linear_sum_assignment(-distances)  # Nous utilisons -distances car linear_sum_assignment trouve la correspondance minimale, mais nous voulons maximiser la similarité\n",
    "\n",
    "# Récupérer les meilleures correspondances et les scores de similarité correspondants\n",
    "matched_answers = [texts_3[j] for j in col_ind]\n",
    "similarity_scores = [distances[i, j] for i, j in zip(row_ind, col_ind)]\n",
    "\n",
    "\n",
    "\n",
    "# Si nécessaire, enregistrer les résultats dans un DataFrame\n",
    "matched_df = pd.DataFrame({\"ID\":  df.iloc[:, 0].values.tolist(),'Question': texts_2,'Answer': texts_3, 'Matched_Text_3': matched_answers, 'Similarity_Score': similarity_scores})\n",
    "matched_df.to_csv('Hangarian sif.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Aucuracy Rate: 0.30%\n"
     ]
    }
   ],
   "source": [
    "column3 = matched_df.iloc[:, 1]\n",
    "column4 = matched_df.iloc[:, 2]\n",
    "\n",
    "\n",
    "consistent_count = 0\n",
    "\n",
    "\n",
    "for value3, value4 in zip(column3, column4):\n",
    "    if value3 == value4:\n",
    "        consistent_count += 1\n",
    "\n",
    "\n",
    "total_rows = len(df)\n",
    "consistency_rate = consistent_count / total_rows\n",
    "\n",
    "print(f\" Aucuracy Rate: {consistency_rate:.2%}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selon les résultats, le résultat obtenu en utilisant le vecteur pondéré TFIDF plus l'algorithme hongrois est le meilleur. L‘algorithme hongrois améliore les matchs comparant utilise les calculs de distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_nightly_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
